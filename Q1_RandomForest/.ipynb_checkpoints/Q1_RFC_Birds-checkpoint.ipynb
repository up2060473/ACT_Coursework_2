{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Random Forest for photo classification using Scikit-learn: An introduction \n",
    "\n",
    "\n",
    "In this notebook, we build a Random Forest Classifier (RFC) in an attempt to classify bird species using scikit-learn and seaborn. \n",
    "\n",
    "Assumed background: Beginners in Machine Learning, some basic python knowledge (although not mandatory).\n",
    "\n",
    "Note, using RFCs to classify images is not the standard method of practice, but is useful to provide insights and comparisons between Convolutional Neural Networks (CNNs) and traditional methods. See ../Q2_CNN/Q2_CNN_Birds.ipynb for CNN methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T07:08:47.728568Z",
     "iopub.status.busy": "2026-01-16T07:08:47.725321Z",
     "iopub.status.idle": "2026-01-16T07:08:47.740994Z",
     "shell.execute_reply": "2026-01-16T07:08:47.738092Z",
     "shell.execute_reply.started": "2026-01-16T07:08:47.728482Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T07:13:09.458248Z",
     "iopub.status.busy": "2026-01-16T07:13:09.457356Z",
     "iopub.status.idle": "2026-01-16T07:13:14.946504Z",
     "shell.execute_reply": "2026-01-16T07:13:14.943435Z",
     "shell.execute_reply.started": "2026-01-16T07:13:09.458161Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.5.2 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from -r ../Dependancies_Q1.txt (line 1)) (1.5.2)\n",
      "Requirement already satisfied: matplotlib==3.7.1 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from -r ../Dependancies_Q1.txt (line 2)) (3.7.1)\n",
      "Requirement already satisfied: seaborn==0.13.2 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from -r ../Dependancies_Q1.txt (line 3)) (0.13.2)\n",
      "Requirement already satisfied: numpy==1.23.5 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from -r ../Dependancies_Q1.txt (line 4)) (1.23.5)\n",
      "Requirement already satisfied: opencv-python in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from -r ../Dependancies_Q1.txt (line 5)) (4.7.0.72)\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from -r ../Dependancies_Q1.txt (line 6)) (1.6.1)\n",
      "Requirement already satisfied: scikit-image==0.19.3 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from -r ../Dependancies_Q1.txt (line 7)) (0.19.3)\n",
      "Requirement already satisfied: torch==1.13.0 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from -r ../Dependancies_Q1.txt (line 8)) (1.13.0)\n",
      "Requirement already satisfied: torchvision==0.14.0 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from -r ../Dependancies_Q1.txt (line 9)) (0.14.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from pandas==1.5.2->-r ../Dependancies_Q1.txt (line 1)) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from pandas==1.5.2->-r ../Dependancies_Q1.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../Dependancies_Q1.txt (line 2)) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../Dependancies_Q1.txt (line 2)) (9.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../Dependancies_Q1.txt (line 2)) (1.0.6)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../Dependancies_Q1.txt (line 2)) (5.10.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../Dependancies_Q1.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../Dependancies_Q1.txt (line 2)) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../Dependancies_Q1.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from matplotlib==3.7.1->-r ../Dependancies_Q1.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from scikit-learn==1.6.1->-r ../Dependancies_Q1.txt (line 6)) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from scikit-learn==1.6.1->-r ../Dependancies_Q1.txt (line 6)) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from scikit-learn==1.6.1->-r ../Dependancies_Q1.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from scikit-image==0.19.3->-r ../Dependancies_Q1.txt (line 7)) (2022.10.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from scikit-image==0.19.3->-r ../Dependancies_Q1.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from scikit-image==0.19.3->-r ../Dependancies_Q1.txt (line 7)) (2.8.8)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from scikit-image==0.19.3->-r ../Dependancies_Q1.txt (line 7)) (2.22.0)\n",
      "Requirement already satisfied: typing_extensions in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from torch==1.13.0->-r ../Dependancies_Q1.txt (line 8)) (4.12.2)\n",
      "Requirement already satisfied: requests in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from torchvision==0.14.0->-r ../Dependancies_Q1.txt (line 9)) (2.32.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib==3.7.1->-r ../Dependancies_Q1.txt (line 2)) (3.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas==1.5.2->-r ../Dependancies_Q1.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from requests->torchvision==0.14.0->-r ../Dependancies_Q1.txt (line 9)) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from requests->torchvision==0.14.0->-r ../Dependancies_Q1.txt (line 9)) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from requests->torchvision==0.14.0->-r ../Dependancies_Q1.txt (line 9)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/idies/mambaforge/envs/py39/lib/python3.9/site-packages (from requests->torchvision==0.14.0->-r ../Dependancies_Q1.txt (line 9)) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "# First, this will ensure that your machine has the corrext packages\n",
    "!pip install -r ../Dependancies_Q1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Image classification is a common and very useful technique in computing, and this skill is becoming increasingly in demand. Whilst Convolutional Neural Networks are a popular choice for image classification, this notebook combined with ../Q2_CNN/Q2_CNN_Birds.ipynb will provides insights into both traditional methods and CNNs.Before we begin, below is a quick overview of the dataset we will be using and how we will build the Random Forest Classifier:\n",
    "\n",
    "Random Forest classifiers (RFCs) are a traditional Machine learning methods that identifies patterns in data by building many decision trees. Each tree infers the data and the model combines these to classify new unseen data. A classification tree splits data at each step by seperating the group by a certain feature. Each path from the root (top) to the leaf (bottom) represents rules used to classsify data. \n",
    "\n",
    "A useful thought experiment: Think about a room full of people. There is a flowchart that asks yes/no questoions - the first question is \"Do you have brown hair?\" - based on the answer you give, you are instructed to move to the left or to the right. There are now 2 groups, those with brown hair and those with not. Both groups now get asked further questions that split them up repeatidly until a certain criteria is met (eg how many people in a single group). This is similar to how RFCs work. \n",
    "\n",
    "\n",
    "Dataset:\n",
    "- We will use the Birds classification dataset from Rahma Sleam, Kaggle. This will remain constant across Q1, Q2 and Q3. \n",
    "- There are 6 types of bird we need to predict, these are called \"classes\" or \"Targets\" as they are what we want to predict givien the features of an image.\n",
    "    - American Goldfinch\n",
    "    - Barn Owl\n",
    "    - Carmine Bee-Eater\n",
    "    - Downy Woodpecker\n",
    "    - Emperor Penguin\n",
    "    - Flamingo\n",
    "- All these birds have their own distinct features as they range over across a wide range of habitats - see link for more detail.\n",
    "- This will be very beneficial for our decision tree classifier. \n",
    "\n",
    "We Will:\n",
    "- Load an image dataset through extracting hog features\n",
    "- Explore how the dataset is distributed among each class (ie, how many images there are for each dataset)\n",
    "    - This will help us limit any biases\n",
    "- Perform Preprocessing\n",
    "- Split data into testing and training\n",
    "- Train a RFC\n",
    "- Evaluate its performance\n",
    "\n",
    "_Link to dataset: https://www.kaggle.com/datasets/rahmasleam/bird-speciees-dataset All rights to their respective owners_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Here, we import the revelent packages to help us visualise and manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T07:08:52.783296Z",
     "iopub.status.busy": "2026-01-16T07:08:52.782464Z",
     "iopub.status.idle": "2026-01-16T07:08:52.796580Z",
     "shell.execute_reply": "2026-01-16T07:08:52.794766Z",
     "shell.execute_reply.started": "2026-01-16T07:08:52.783238Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skimage.feature import hog\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction using HOGs\n",
    "Traditional ML models cant work directly with images, therefore we use Histograms of Orientated Gradients (HOGs) to interpret edges, texture, shape and convert these findings to numerical feature vectors that then can be put into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T07:08:54.841179Z",
     "iopub.status.busy": "2026-01-16T07:08:54.840335Z",
     "iopub.status.idle": "2026-01-16T07:08:54.852607Z",
     "shell.execute_reply": "2026-01-16T07:08:54.850242Z",
     "shell.execute_reply.started": "2026-01-16T07:08:54.841121Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_hog_features(image):\n",
    "    ## Take a single image as an input and extractes its hog features\n",
    "    ## hog(...): Histogram of Orientated Gradients (HOG)\n",
    "    ## orientations = 9: Gradient directions are divided into 9 bins\n",
    "    ## Pixel per cell = (8,8): Each cell is used to compute a HOG\n",
    "    hog_features = hog(\n",
    "        image,\n",
    "        orientations = 9,\n",
    "        pixels_per_cell = (8, 8),\n",
    "        cells_per_block = (2, 2),\n",
    "        visualize=False\n",
    "    )\n",
    "    return hog_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:31:06.368456Z",
     "iopub.status.busy": "2026-01-15T12:31:06.364605Z",
     "iopub.status.idle": "2026-01-15T12:31:06.403957Z",
     "shell.execute_reply": "2026-01-15T12:31:06.401030Z",
     "shell.execute_reply.started": "2026-01-15T12:31:06.368358Z"
    },
    "tags": []
   },
   "source": [
    "Now, we loop through each class folder, resize them to be consistent with CNN notebook, then extract the HOG features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T07:08:56.801976Z",
     "iopub.status.busy": "2026-01-16T07:08:56.801116Z",
     "iopub.status.idle": "2026-01-16T07:08:56.820415Z",
     "shell.execute_reply": "2026-01-16T07:08:56.817998Z",
     "shell.execute_reply.started": "2026-01-16T07:08:56.801918Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_extract_features(directory):\n",
    "    ## loads and extracts features\n",
    "    ## Directory: path to dataset\n",
    "    ## Create empty lists\n",
    "        ## x = list of HOG features\n",
    "        ## y = list of class labels\n",
    "    x, y = [], []\n",
    "    for label, class_name in enumerate(os.listdir(directory)):\n",
    "        ## Loops through each folder in the directory\n",
    "        ## os.listdir(directory) returns folder names (each folder is a class)\n",
    "        ## enumerate: changes class label from string to integer\n",
    "        ## class_dir: stores directory to each class\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            ## Loops through each image in the class folder\n",
    "            ## image_path: Stores the full direcotry\n",
    "            ## img = cv2....: Reads the image using OpenCV, result numpy array\n",
    "            ## if img is None: Skips any images that fail to load\n",
    "            ## img_resized = resizes images to 128, 128 to be fair with CNN Notebook\n",
    "            ## hog_features: Calls the function defined in the cell above\n",
    "            ## Appends x and y with features and labels \n",
    "            image_path = os.path.join(class_dir, filename)\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img_resized = cv2.resize(img, (128,128))\n",
    "            img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY) \n",
    "            ## HOG is better suited for grayscale images and sees better performance this way\n",
    "            ## Hence the deviation from the color images used in the CNNs\n",
    "            hog_features = extract_hog_features(img_gray)\n",
    "            x.append(hog_features)\n",
    "            y.append(label)\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "Now we can use the two helper functions defined above to load and extract the hog features ready to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T07:09:53.853882Z",
     "iopub.status.busy": "2026-01-16T07:09:53.852892Z",
     "iopub.status.idle": "2026-01-16T07:10:18.936406Z",
     "shell.execute_reply": "2026-01-16T07:10:18.933856Z",
     "shell.execute_reply.started": "2026-01-16T07:09:53.853818Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Finished laoding data\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../birds_dataset/Bird Speciees Dataset/\"\n",
    "x, y = load_and_extract_features(data_path)\n",
    "print(\">>> Finished loading data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:33:17.682610Z",
     "iopub.status.busy": "2026-01-15T12:33:17.680489Z",
     "iopub.status.idle": "2026-01-15T12:33:17.699208Z",
     "shell.execute_reply": "2026-01-15T12:33:17.696810Z",
     "shell.execute_reply.started": "2026-01-15T12:33:17.682547Z"
    },
    "tags": []
   },
   "source": [
    "## Train / Test Split\n",
    "\n",
    "We now will split the dataset into a 80% : 20% split for training and testing\n",
    "- 80% of the total data will be used to train the model\n",
    "- 20% of the total data will be used to test the model and produce accuracy scores\n",
    "\n",
    "This is common practice in ML because:\n",
    "- It can help to ensure that the RFC is not simply memorising patterns from training. This helps to prevent overfitting:\n",
    "- Overfitting occurs when the RFC fits the training data too closely, that it cannot provide good predictions on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T07:10:22.059774Z",
     "iopub.status.busy": "2026-01-16T07:10:22.058789Z",
     "iopub.status.idle": "2026-01-16T07:10:22.185126Z",
     "shell.execute_reply": "2026-01-16T07:10:22.182611Z",
     "shell.execute_reply.started": "2026-01-16T07:10:22.059681Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    ## Using sklearn's built in data splitter\n",
    "    ## Test size = 20% (Consistent with CNN)\n",
    "    ## Train size = 80% (consistent with CNN)\n",
    "    ## Random state is set to promote reproducability\n",
    "    ## Stratify ensures that class distribution in train and test sets matches original\n",
    "    x, y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 42,\n",
    "    stratify = y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "RFC models work more efficiently when the images are scaled, therefore we use standard scaler on HOG feature so that mean = 0 and std = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T07:10:25.985278Z",
     "iopub.status.busy": "2026-01-16T07:10:25.984424Z",
     "iopub.status.idle": "2026-01-16T07:10:26.517939Z",
     "shell.execute_reply": "2026-01-16T07:10:26.515216Z",
     "shell.execute_reply.started": "2026-01-16T07:10:25.985218Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Using the standard scaler from scikit-learn\n",
    "## Normalises data so that mean = 0 and standard deviation = 1\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Random Forest Classifier\n",
    "\n",
    "Now that the data has been preprocessed, we can now define our model arcitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T07:10:29.495550Z",
     "iopub.status.busy": "2026-01-16T07:10:29.494679Z",
     "iopub.status.idle": "2026-01-16T07:10:34.314887Z",
     "shell.execute_reply": "2026-01-16T07:10:34.312941Z",
     "shell.execute_reply.started": "2026-01-16T07:10:29.495493Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training Random Forest Classifier\n",
      ">>> Training complete\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(\n",
    "    ## Definition of the Random Forest Classifier\n",
    "    ## n_estimators = 100: Number of trees in the forest\n",
    "    ## n_jobs = 1: number of CPU cores\n",
    "    ## max_depth: Limits the depth of each decision tree, prevents overfitting\n",
    "    n_estimators = 100,\n",
    "    random_state = 42,\n",
    "    n_jobs = 1,\n",
    "    max_depth = 5\n",
    ")\n",
    "\n",
    "## Training the random forest classifier on the training dataset\n",
    "print(\">>> Training Random Forest Classifier\")\n",
    "rf_clf.fit(x_train_scaled, y_train)\n",
    "print(\">>> Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing and evaluation\n",
    "Now that our model is trained, its time to test it on the testing dataset (Remember we split the data into 80% training and 20% testing). We will print out the accuract scores for each class in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T07:10:35.923882Z",
     "iopub.status.busy": "2026-01-16T07:10:35.923006Z",
     "iopub.status.idle": "2026-01-16T07:10:35.981145Z",
     "shell.execute_reply": "2026-01-16T07:10:35.978883Z",
     "shell.execute_reply.started": "2026-01-16T07:10:35.923823Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Testing model\n",
      ">>> Testing completed, results follow\n"
     ]
    }
   ],
   "source": [
    "## Testing Random Forest Classifier\n",
    "## y_pred: array of predicted classes\n",
    "## accuracy_score = accuracy = 100* (# Correct predictions / # Total predictions)\n",
    "print(\">>> Testing model\")\n",
    "y_pred = rf_clf.predict(x_test_scaled)\n",
    "print(\">>> Testing completed, results follow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting results and evaluating performance\n",
    "\n",
    "Now that we have trained our Random Forest Classifier, we can evaluate how well it performed during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T07:10:38.355067Z",
     "iopub.status.busy": "2026-01-16T07:10:38.354242Z",
     "iopub.status.idle": "2026-01-16T07:10:38.365035Z",
     "shell.execute_reply": "2026-01-16T07:10:38.362669Z",
     "shell.execute_reply.started": "2026-01-16T07:10:38.355010Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining some variables that will help us to interprest results\n",
    "## target_names: names of each class\n",
    "data_path = \"../birds_dataset/Bird Speciees Dataset/\"\n",
    "target_names = sorted(os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report\n",
    "\n",
    "A classification report gives a summary on how the Random Forest Classifier performed for each class.\n",
    "\n",
    "Precision = # True Positive / # True and False Positives. Low precision would mean tha tthe model often confuses other birds for the true result\n",
    "\n",
    "Recall = # True Positive / # True Positive + # False Negative. High recall would mean that the model only gets a few false negatives (does not miss many real exampled)\n",
    "\n",
    "F1-Score = 2 x Precision*Recall / Precision+Recall. Balance between precision and recall\n",
    "\n",
    "Support = How many images of birds where in the test set. Useful to elimate biases if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T07:10:40.394835Z",
     "iopub.status.busy": "2026-01-16T07:10:40.393923Z",
     "iopub.status.idle": "2026-01-16T07:10:40.444815Z",
     "shell.execute_reply": "2026-01-16T07:10:40.442915Z",
     "shell.execute_reply.started": "2026-01-16T07:10:40.394768Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Classification Report\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "AMERICAN GOLDFINCH       0.59      0.66      0.62        29\n",
      "          BARN OWL       0.56      0.38      0.45        26\n",
      " CARMINE BEE-EATER       0.52      0.42      0.47        26\n",
      "  DOWNY WOODPECKER       0.50      0.61      0.55        28\n",
      "   EMPEROR PENGUIN       0.71      0.79      0.75        28\n",
      "          FLAMINGO       0.41      0.42      0.42        26\n",
      "\n",
      "          accuracy                           0.55       163\n",
      "         macro avg       0.55      0.55      0.54       163\n",
      "      weighted avg       0.55      0.55      0.55       163\n",
      "\n",
      "Overall accuracy: 55.21%\n"
     ]
    }
   ],
   "source": [
    "## Classification Report\n",
    "## class_report: uses the built in classification report\n",
    "## accuracy: uses accuracy_score to obtain the overall perfomance: 100*(Correct predictions / Total predictions)\n",
    "class_report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "print(\">>> Classification Report\\n\")\n",
    "print(class_report)\n",
    "accuracy = 100*accuracy_score(y_test, y_pred)\n",
    "print(f\"Overall accuracy: {accuracy:.4}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "American Goldfinch\n",
    "- High recall - Catches most American Goldfinches\n",
    "- Sometimes predicts goldfinches when it is not\n",
    "\n",
    "Barn Owl\n",
    "- Low recall - Misses many Barn Owls\n",
    "- Low precision - May get confused with birds of similar feature\n",
    "\n",
    "Carmine Bee-eater\n",
    "- High precision - catches most Bee-eater\n",
    "- Fairly reasonable recall - Misses some bee eaters\n",
    "\n",
    "Downy Woodpecker\n",
    "- Fairly high recall - catches most woodpeckers\n",
    "- Low precision - confuses many other birds as woodpeckers\n",
    "\n",
    "Emperor Penguin\n",
    "- High precision and recall due to distinct features\n",
    "\n",
    "Flamingo\n",
    "- Moderaate precision and recall.\n",
    "- Struggles to distinguish flamingoes from rest of set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "A confusion matrix shows the number of true positives, true negatives, false positives and false negatives between testing and training\n",
    "\n",
    "Rows: True classes in order of American Goldfinch, Barn Owl, Carmine Bee-eater, Downy Woodpecker, Emperor Penguin and Flamingo\n",
    "Columns: Prediction classes of the same order as above\n",
    "\n",
    "The number in the leading diagnonal show correct predictions, any other entry is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T07:10:48.448081Z",
     "iopub.status.busy": "2026-01-16T07:10:48.447263Z",
     "iopub.status.idle": "2026-01-16T07:10:48.464525Z",
     "shell.execute_reply": "2026-01-16T07:10:48.462341Z",
     "shell.execute_reply.started": "2026-01-16T07:10:48.448022Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Confusion matrix\n",
      "\n",
      "[[19  1  3  2  2  2]\n",
      " [ 1 10  2  9  1  3]\n",
      " [ 7  1 11  2  2  3]\n",
      " [ 1  4  2 17  1  3]\n",
      " [ 0  0  0  1 22  5]\n",
      " [ 4  2  3  3  3 11]]\n"
     ]
    }
   ],
   "source": [
    "## Confusion matrix\n",
    "## conf_matrix uses the built in function to compute the confusion matrix between y_test and y_pred\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\">>> Confusion matrix\\n\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretting the confusion matrix\n",
    "HOG focuses on Edges, Shapes and main silhouettes and largely ignores fine texture and color. It captures couarse texture but not fine-grain texture or spatial hierarchies. Therefore, the model performs better on classes that have very distinct features such as the Emperor Penguin. Classes with more subtle features can become confused, for example Barn Owls are often mistaken for Downy Woodpackers, suggesting that edge-based features alone are often insufficient for fine image classification. This is a key limitation for Random Forest Classifiers used with HOG extraction. \n",
    "\n",
    "This limitation is well documented alongside the fact that Random Forests do not learn hierarchial visual features. However, there is an alternative method - Convolutional Neural Networks, To view how we approach this, see notebook ../Q2_CNN/Q2_CNN_Birds.ipynb ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References used in this notebook\n",
    "\n",
    "**Code inspired by**\n",
    "1. Class Notes and lecture notes: 24th December to 5th December, Coleman K.\n",
    "    - General workflow\n",
    "    - Specific inspiration for the definition of \n",
    "        - Transforms\n",
    "        - Loading data\n",
    "        - General information regarding Radom Forest Classifiers\n",
    "2. Interpreting Random Forest Results, Geeks for Geeks\n",
    "    - Aid in analysing results from the Random Forest\n",
    "    - Link: https://www.geeksforgeeks.org/machine-learning/interpreting-random-forest-classification-results/\n",
    "3. HOG Feature Visualisation\n",
    "    - Research into what HOG is and how to use it - Justification for using gray scale\n",
    "    - Link: https://www.geeksforgeeks.org/machine-learning/hog-feature-visualization-in-python-using-skimage/\n",
    "4. Random Forest Classifiers, six sided dice\n",
    "    - Research into the justification for using RFCs for image classification\n",
    "    - Aid in coding general workflow\n",
    "    - Link: https://www.sixsideddice.com/Blog/MLByExample/RandomForestsForImageClassification.html\n",
    "5. Random Forest for Image Classification using open cv\n",
    "    - How to use open cv\n",
    "    - Link: https://www.geeksforgeeks.org/machine-learning/random-forest-for-image-classification-using-opencv/\n",
    "6. Random Forest Classifier using scikit learn\n",
    "    - General workfloy - how to run the forest classifier\n",
    "    - Link:https://www.geeksforgeeks.org/dsa/random-forest-classifier-using-scikit-learn/\n",
    "7. How to import Kaggle datasets\n",
    "    - Research into how to import kaggle datasets\n",
    "    - Link: https://www.geeksforgeeks.org/python/how-to-import-kaggle-datasets-directly-into-google-colab/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
